# -*- coding: utf-8 -*-
"""AMZ_Ads_Reports.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dq4twM8SIiF4x8y1vZICUExsAboznVTN
"""

### fetch_amazon_ads.py
import requests
import os
import json
import pandas as pd
from google.cloud import bigquery
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

ACCESS_TOKEN = os.getenv("AMAZON_ADS_ACCESS_TOKEN")
REFRESH_TOKEN = os.getenv("AMAZON_ADS_REFRESH_TOKEN")
API_URL = "https://advertising-api.amazon.com/v2/reports"

if not ACCESS_TOKEN:
    raise ValueError("Missing AMAZON_ADS_ACCESS_TOKEN in environment variables.")
if not REFRESH_TOKEN:
    raise ValueError("Missing AMAZON_ADS_REFRESH_TOKEN in environment variables.")

# Fetch report from Amazon Ads API
def fetch_ads_report():
    headers = {
        "Authorization": f"Bearer {ACCESS_TOKEN}",
        "Content-Type": "application/json",
    }
    payload = {
        "reportDate": "2023-12-01",  # Ensure proper date format
        "metrics": ["campaignId,startDate,endDate,impressions,asins,clicks,spend,sales"]
    }
    response = requests.post(API_URL, headers=headers, json=payload)

    if response.status_code == 200:
        response_data = response.json()
        if not response_data:
            print("Error: API response is empty.")
            return None

        with open("ads_report.json", "w") as f:
            json.dump(response_data, f)
        print("Report fetched and saved successfully.")
        return "ads_report.json"
    else:
        print("Failed to fetch report:", response.text)
        return None

# Initialize BigQuery client
client = bigquery.Client()

# Configure BigQuery Dataset and Table
DATASET_ID = "my_dataset"
TABLE_ID = "ads_performance"

# Load and clean data
def load_and_clean_data(file_path):
    with open(file_path, "r") as f:
        raw_data = json.load(f)

    if "data" not in raw_data:
        raise ValueError(f"Invalid JSON format: Missing 'data' key in {file_path}")

    df = pd.DataFrame(raw_data["data"])

    if "reportDate" in raw_data:
        df["report_date"] = pd.to_datetime(raw_data["reportDate"])

    # Save as CSV
    csv_file_path = file_path.replace(".json", ".csv")
    df.to_csv(csv_file_path, index=False)

    # Save as Excel
    excel_file_path = file_path.replace(".json", ".xlsx")
    df.to_excel(excel_file_path, index=False)

    return csv_file_path  # Return CSV file path

# Upload CSV to BigQuery
def upload_to_bigquery(csv_file_path):
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,  # Skip header row
        autodetect=True  # Auto-detect schema
    )

    with open(csv_file_path, "rb") as f:
        job = client.load_table_from_file(f, table_ref, job_config=job_config)

    job.result()  # Wait for job to complete
    print(f"Data uploaded successfully to {DATASET_ID}.{TABLE_ID}")

# Main execution
file_path = fetch_ads_report()
if file_path:
    csv_file_path = load_and_clean_data(file_path)
    upload_to_bigquery(csv_file_path)